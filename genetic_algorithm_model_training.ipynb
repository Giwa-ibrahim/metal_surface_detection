{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygeZ2NlaWYF7","executionInfo":{"status":"ok","timestamp":1757151083147,"user_tz":-60,"elapsed":22124,"user":{"displayName":"Sikirulahi Abdulkareem","userId":"07450201339681726284"}},"outputId":"0686125d-fe99-432c-a8f1-f69bfe77af5a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ho5tp3pXWX8l","executionInfo":{"status":"ok","timestamp":1757151091108,"user_tz":-60,"elapsed":7958,"user":{"displayName":"Sikirulahi Abdulkareem","userId":"07450201339681726284"}},"outputId":"eeaee5e8-cf80-41ea-d0f9-8d78437fbc07"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","source":["from tqdm.auto import tqdm\n","import pandas as pd\n","import numpy as np\n","import shutil\n","import torch\n","from torch import nn\n","from torchinfo import summary\n","import cv2\n","from torch.utils.data import Dataset\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import os\n","import albumentations as A\n","import cv2\n","import zipfile\n","from torchvision import transforms, datasets\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from time import perf_counter as timer\n","import math\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","import numpy as np\n","from typing import Callable, Tuple, Optional, Dict, Any\n","import matplotlib.pyplot as plt\n","\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from typing import Dict, Any, List, Tuple, Optional\n","import random\n","from sklearn.model_selection import ParameterSampler\n","from scipy.stats import uniform, loguniform, randint\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from datetime import datetime\n","import json"],"metadata":{"id":"Tah-y_rWWX4-","executionInfo":{"status":"ok","timestamp":1757151112821,"user_tz":-60,"elapsed":21715,"user":{"displayName":"Sikirulahi Abdulkareem","userId":"07450201339681726284"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Your existing data loading code:\n","train_data_path = \"/content/drive/MyDrive/Metal Detection Data/Processed_Data/train\"\n","valid_data_path = \"/content/drive/MyDrive/Metal Detection Data/Processed_Data/valid\"\n","test_data_path = \"/content/drive/MyDrive/Metal Detection Data/Processed_Data/test\"\n","\n","transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","])\n","\n","train_data = datasets.ImageFolder(train_data_path, transform=transform)\n","valid_data = datasets.ImageFolder(valid_data_path, transform=transform)\n","test_data = datasets.ImageFolder(test_data_path, transform=transform)\n","\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n","valid_loader = DataLoader(valid_data, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n","\n","class_names = train_data.classes\n","class_dict = train_data.class_to_idx"],"metadata":{"id":"mOa2OEvzWX3s","executionInfo":{"status":"ok","timestamp":1757151119306,"user_tz":-60,"elapsed":6483,"user":{"displayName":"Sikirulahi Abdulkareem","userId":"07450201339681726284"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class TinyVGG_v2(nn.Module):\n","    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n","        super().__init__()\n","        self.conv_block_1 = nn.Sequential(\n","            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.conv_block_2 = nn.Sequential(\n","            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(in_features=hidden_units * 64 * 64, out_features=output_shape),\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.conv_block_1(x)\n","        x = self.conv_block_2(x)\n","        x = self.classifier(x)\n","        return x\n","\n","\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","\n","model = TinyVGG_v2(\n","    input_shape=3,\n","    hidden_units=64,\n","    output_shape=len(class_names)\n",").to(device)\n","\n","print(f\"Fixed model parameters: {sum(p.numel() for p in model.parameters())}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEDXyzzxWXzO","executionInfo":{"status":"ok","timestamp":1757151119931,"user_tz":-60,"elapsed":623,"user":{"displayName":"Sikirulahi Abdulkareem","userId":"07450201339681726284"}},"outputId":"b9a22f8d-53a8-4171-b2ad-a0df9d23a960"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Fixed model parameters: 1685446\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"ivTHE1N2VMRM","executionInfo":{"status":"ok","timestamp":1757151119953,"user_tz":-60,"elapsed":23,"user":{"displayName":"Sikirulahi Abdulkareem","userId":"07450201339681726284"}}},"outputs":[],"source":["class GeneticAlgorithm:\n","    \"\"\"\n","    Genetic Algorithm implementation for hyperparameter optimization\n","    \"\"\"\n","    def __init__(self,\n","                 objective_function,\n","                 dimension,\n","                 population_size=20,\n","                 max_generations=50,\n","                 bounds=(0, 1),\n","                 crossover_rate=0.8,\n","                 mutation_rate=0.1,\n","                 tournament_size=3,\n","                 elite_size=2,\n","                 random_seed=None):\n","\n","        self.objective_function = objective_function\n","        self.dimension = dimension\n","        self.population_size = population_size\n","        self.max_generations = max_generations\n","        self.bounds = bounds\n","        self.crossover_rate = crossover_rate\n","        self.mutation_rate = mutation_rate\n","        self.tournament_size = tournament_size\n","        self.elite_size = elite_size\n","\n","        if random_seed is not None:\n","            np.random.seed(random_seed)\n","            random.seed(random_seed)\n","            torch.manual_seed(random_seed)\n","\n","        # Initialize population\n","        self.population = np.random.uniform(bounds[0], bounds[1],\n","                                          (population_size, dimension))\n","        self.fitness_scores = np.full(population_size, np.inf)\n","\n","        # Track best solution\n","        self.best_individual = None\n","        self.best_fitness = np.inf\n","        self.convergence_history = []\n","\n","    def evaluate_population(self):\n","        \"\"\"Evaluate fitness for entire population\"\"\"\n","        for i in range(self.population_size):\n","            fitness = self.objective_function(self.population[i])\n","            self.fitness_scores[i] = fitness\n","\n","            if fitness < self.best_fitness:\n","                self.best_fitness = fitness\n","                self.best_individual = self.population[i].copy()\n","\n","    def tournament_selection(self):\n","        \"\"\"Tournament selection for parent selection\"\"\"\n","        selected_parents = []\n","\n","        for _ in range(self.population_size - self.elite_size):\n","            # Tournament selection\n","            tournament_indices = np.random.choice(self.population_size,\n","                                                self.tournament_size, replace=False)\n","            tournament_fitness = self.fitness_scores[tournament_indices]\n","            winner_idx = tournament_indices[np.argmin(tournament_fitness)]\n","            selected_parents.append(self.population[winner_idx].copy())\n","\n","        return np.array(selected_parents)\n","\n","    def blend_crossover(self, parent1, parent2, alpha=0.5):\n","        \"\"\"BLX-α (Blend Crossover) for real-valued genes\"\"\"\n","        child1 = np.zeros(self.dimension)\n","        child2 = np.zeros(self.dimension)\n","\n","        for i in range(self.dimension):\n","            if np.random.random() < self.crossover_rate:\n","                # BLX-α crossover\n","                min_val = min(parent1[i], parent2[i])\n","                max_val = max(parent1[i], parent2[i])\n","                diff = max_val - min_val\n","\n","                lower_bound = max(self.bounds[0], min_val - alpha * diff)\n","                upper_bound = min(self.bounds[1], max_val + alpha * diff)\n","\n","                child1[i] = np.random.uniform(lower_bound, upper_bound)\n","                child2[i] = np.random.uniform(lower_bound, upper_bound)\n","            else:\n","                child1[i] = parent1[i]\n","                child2[i] = parent2[i]\n","\n","        return child1, child2\n","\n","    def gaussian_mutation(self, individual, sigma=0.1):\n","        \"\"\"Gaussian mutation for real-valued genes\"\"\"\n","        mutated = individual.copy()\n","\n","        for i in range(self.dimension):\n","            if np.random.random() < self.mutation_rate:\n","                # Add Gaussian noise\n","                mutation_strength = sigma * (self.bounds[1] - self.bounds[0])\n","                mutated[i] += np.random.normal(0, mutation_strength)\n","\n","                # Ensure bounds\n","                mutated[i] = np.clip(mutated[i], self.bounds[0], self.bounds[1])\n","\n","        return mutated\n","\n","    def adaptive_parameters(self, generation):\n","        \"\"\"Adaptive mutation rate - decreases over generations\"\"\"\n","        initial_mutation = self.mutation_rate\n","        final_mutation = self.mutation_rate * 0.1\n","\n","        # Linear decay\n","        adaptive_mutation = initial_mutation - (initial_mutation - final_mutation) * (generation / self.max_generations)\n","        return adaptive_mutation\n","\n","    def create_new_generation(self, generation):\n","        \"\"\"Create new generation through selection, crossover, and mutation\"\"\"\n","        new_population = []\n","\n","        # Elitism - keep best individuals\n","        elite_indices = np.argsort(self.fitness_scores)[:self.elite_size]\n","        for idx in elite_indices:\n","            new_population.append(self.population[idx].copy())\n","\n","        # Tournament selection\n","        parents = self.tournament_selection()\n","\n","        # Crossover and mutation\n","        adaptive_mutation_rate = self.adaptive_parameters(generation)\n","\n","        for i in range(0, len(parents), 2):\n","            if i + 1 < len(parents):\n","                parent1 = parents[i]\n","                parent2 = parents[i + 1]\n","\n","                # Crossover\n","                child1, child2 = self.blend_crossover(parent1, parent2)\n","\n","                # Mutation\n","                self.mutation_rate = adaptive_mutation_rate\n","                child1 = self.gaussian_mutation(child1)\n","                child2 = self.gaussian_mutation(child2)\n","\n","                new_population.extend([child1, child2])\n","            else:\n","                # Odd number of parents, mutate the last one\n","                child = self.gaussian_mutation(parents[i])\n","                new_population.append(child)\n","\n","        # Ensure population size\n","        self.population = np.array(new_population[:self.population_size])\n","\n","    def optimize(self, verbose=True):\n","        \"\"\"Run Genetic Algorithm optimization\"\"\"\n","        if verbose:\n","            print(\"Starting Genetic Algorithm optimization...\")\n","            print(f\"Population size: {self.population_size}\")\n","            print(f\"Dimensions: {self.dimension}\")\n","            print(f\"Max generations: {self.max_generations}\")\n","            print(f\"Crossover rate: {self.crossover_rate}\")\n","            print(f\"Mutation rate: {self.mutation_rate}\")\n","            print(\"-\" * 50)\n","\n","        # Initial evaluation\n","        self.evaluate_population()\n","        self.convergence_history.append(self.best_fitness)\n","\n","        if verbose:\n","            print(f\"Generation   0: Best fitness = {self.best_fitness:.6f}\")\n","\n","        # Evolution loop\n","        for generation in range(1, self.max_generations + 1):\n","            # Create new generation\n","            self.create_new_generation(generation - 1)\n","\n","            # Evaluate new population\n","            self.evaluate_population()\n","            self.convergence_history.append(self.best_fitness)\n","\n","            if verbose and generation % 5 == 0:\n","                avg_fitness = np.mean(self.fitness_scores)\n","                print(f\"Generation {generation:3d}: Best = {self.best_fitness:.6f}, \"\n","                      f\"Avg = {avg_fitness:.6f}, \"\n","                      f\"Std = {np.std(self.fitness_scores):.6f}\")\n","\n","        if verbose:\n","            print(\"-\" * 50)\n","            print(f\"Evolution completed!\")\n","            print(f\"Best fitness: {self.best_fitness:.6f}\")\n","\n","        return {\n","            'best_individual': self.best_individual,\n","            'best_fitness': self.best_fitness,\n","            'convergence_history': self.convergence_history,\n","            'final_population': self.population,\n","            'final_fitness_scores': self.fitness_scores\n","        }"]},{"cell_type":"code","source":["class GA_CNN_Trainer:\n","    \"\"\"CNN Trainer with Genetic Algorithm\"\"\"\n","\n","    def __init__(self,\n","                 train_loader,\n","                 valid_loader,\n","                 test_loader,\n","                 num_classes,\n","                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n","\n","        self.train_loader = train_loader\n","        self.valid_loader = valid_loader\n","        self.test_loader = test_loader\n","        self.num_classes = num_classes\n","        self.device = device\n","\n","        # Parameter bounds for GA optimization\n","        self.param_bounds = {\n","            'learning_rate': (1e-5, 1e-1),    # log scale\n","            'hidden_units': (8, 128),         # linear scale\n","            'dropout_rate': (0.0, 0.7),       # linear scale\n","            'weight_decay': (1e-6, 1e-2),     # log scale\n","            'batch_norm': (0, 1)              # binary choice (0 or 1)\n","        }\n","\n","    def vector_to_params(self, vector):\n","        \"\"\"Convert GA vector to CNN parameters\"\"\"\n","        params = {}\n","\n","        # Learning rate (log scale)\n","        params['learning_rate'] = 10 ** (vector[0] * (np.log10(self.param_bounds['learning_rate'][1]) -\n","                                                      np.log10(self.param_bounds['learning_rate'][0])) +\n","                                        np.log10(self.param_bounds['learning_rate'][0]))\n","\n","        # Hidden units (linear scale)\n","        params['hidden_units'] = int(vector[1] * (self.param_bounds['hidden_units'][1] -\n","                                                 self.param_bounds['hidden_units'][0]) +\n","                                    self.param_bounds['hidden_units'][0])\n","\n","        # Dropout rate (linear scale)\n","        params['dropout_rate'] = vector[2] * (self.param_bounds['dropout_rate'][1] -\n","                                             self.param_bounds['dropout_rate'][0]) + \\\n","                                self.param_bounds['dropout_rate'][0]\n","\n","        # Weight decay (log scale)\n","        params['weight_decay'] = 10 ** (vector[3] * (np.log10(self.param_bounds['weight_decay'][1]) -\n","                                                     np.log10(self.param_bounds['weight_decay'][0])) +\n","                                       np.log10(self.param_bounds['weight_decay'][0]))\n","\n","        # Batch normalization (binary choice)\n","        params['use_batch_norm'] = vector[4] > 0.5\n","\n","        return params\n","\n","    def create_model_with_options(self, hidden_units, dropout_rate, use_batch_norm):\n","        \"\"\"Create TinyVGG model with optional batch normalization and dropout\"\"\"\n","\n","        if use_batch_norm:\n","            # Model with batch normalization\n","            conv_block_1 = nn.Sequential(\n","                nn.Conv2d(in_channels=3, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n","                nn.BatchNorm2d(hidden_units),\n","                nn.ReLU(),\n","                nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n","                nn.BatchNorm2d(hidden_units),\n","                nn.ReLU(),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","            )\n","            conv_block_2 = nn.Sequential(\n","                nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(hidden_units),\n","                nn.ReLU(),\n","                nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(hidden_units),\n","                nn.ReLU(),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","            )\n","        else:\n","            # Standard model without batch normalization\n","            conv_block_1 = nn.Sequential(\n","                nn.Conv2d(in_channels=3, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n","                nn.ReLU(),\n","                nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n","                nn.ReLU(),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","            )\n","            conv_block_2 = nn.Sequential(\n","                nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","                nn.ReLU(),\n","                nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n","                nn.ReLU(),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","            )\n","\n","        # Create model\n","        model = nn.Module()\n","        model.conv_block_1 = conv_block_1\n","        model.conv_block_2 = conv_block_2\n","        model.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(in_features=hidden_units * 64 * 64, out_features=self.num_classes),\n","        )\n","\n","        # Define forward method\n","        def forward(x):\n","            x = model.conv_block_1(x)\n","            x = model.conv_block_2(x)\n","            x = model.classifier(x)\n","            return x\n","\n","        model.forward = forward\n","        return model.to(self.device)\n","\n","    def train_and_evaluate(self, params, epochs=8):\n","        \"\"\"Train CNN with given parameters and return validation loss\"\"\"\n","        # Create model with options\n","        model = self.create_model_with_options(\n","            params['hidden_units'],\n","            params['dropout_rate'],\n","            params['use_batch_norm']\n","        )\n","\n","        # Setup training\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(model.parameters(),\n","                                   lr=params['learning_rate'],\n","                                   weight_decay=params['weight_decay'])\n","\n","        # Training loop\n","        model.train()\n","        for epoch in range(epochs):\n","            epoch_loss = 0\n","            batch_count = 0\n","\n","            for batch_idx, (data, target) in enumerate(self.train_loader):\n","                if batch_idx > 30:  # Limit training for faster GA iterations\n","                    break\n","\n","                data, target = data.to(self.device), target.to(self.device)\n","                optimizer.zero_grad()\n","                output = model(data)\n","                loss = criterion(output, target)\n","                loss.backward()\n","                optimizer.step()\n","\n","                epoch_loss += loss.item()\n","                batch_count += 1\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        val_correct = 0\n","        val_total = 0\n","\n","        with torch.no_grad():\n","            for data, target in self.valid_loader:\n","                data, target = data.to(self.device), target.to(self.device)\n","                output = model(data)\n","                val_loss += criterion(output, target).item()\n","                pred = output.argmax(dim=1)\n","                val_correct += pred.eq(target).sum().item()\n","                val_total += target.size(0)\n","\n","        val_accuracy = val_correct / val_total\n","        avg_val_loss = val_loss / len(self.valid_loader)\n","\n","        # Return fitness (what GA minimizes) with penalty for low accuracy\n","        fitness = avg_val_loss + (1 - val_accuracy) * 0.4\n","        return fitness, model, val_accuracy\n","\n","    def objective_function(self, vector):\n","        \"\"\"Objective function for GA\"\"\"\n","        # Ensure vector is in bounds [0, 1]\n","        vector = np.clip(vector, 0, 1)\n","        params = self.vector_to_params(vector)\n","        fitness, _, _ = self.train_and_evaluate(params, epochs=5)  # Short training for GA\n","        return fitness\n","\n","    def run_optimization(self,\n","                        population_size=20,\n","                        max_generations=30,\n","                        crossover_rate=0.8,\n","                        mutation_rate=0.15,\n","                        tournament_size=3,\n","                        elite_size=2,\n","                        verbose=True):\n","        \"\"\"Run GA optimization\"\"\"\n","\n","        ga = GeneticAlgorithm(\n","            objective_function=self.objective_function,\n","            dimension=5,  # 5 parameters to optimize (including batch_norm)\n","            population_size=population_size,\n","            max_generations=max_generations,\n","            bounds=(0, 1),  # Normalized bounds\n","            crossover_rate=crossover_rate,\n","            mutation_rate=mutation_rate,\n","            tournament_size=tournament_size,\n","            elite_size=elite_size,\n","            random_seed=42\n","        )\n","\n","        result = ga.optimize(verbose=verbose)\n","\n","        # Get best parameters\n","        best_params = self.vector_to_params(result['best_individual'])\n","\n","        # Train final model with best parameters\n","        print(\"\\nTraining final model with best parameters...\")\n","        print(f\"Best parameters: {best_params}\")\n","\n","        final_fitness, final_model, final_accuracy = self.train_and_evaluate(\n","            best_params, epochs=25\n","        )\n","\n","        return {\n","            'best_params': best_params,\n","            'final_model': final_model,\n","            'final_accuracy': final_accuracy,\n","            'convergence_history': result['convergence_history'],\n","            'final_population': result['final_population'],\n","            'diversity_stats': {\n","                'final_std': np.std(result['final_fitness_scores']),\n","                'final_mean': np.mean(result['final_fitness_scores'])\n","            }\n","        }\n","\n","    def test_model(self, model):\n","        \"\"\"Test the final model\"\"\"\n","        model.eval()\n","        test_correct = 0\n","        test_total = 0\n","\n","        with torch.no_grad():\n","            for data, target in self.test_loader:\n","                data, target = data.to(self.device), target.to(self.device)\n","                output = model(data)\n","                pred = output.argmax(dim=1)\n","                test_correct += pred.eq(target).sum().item()\n","                test_total += target.size(0)\n","\n","        test_accuracy = test_correct / test_total\n","        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n","        return test_accuracy\n","\n","\n","def run_ga_training(train_loader, valid_loader, test_loader, num_classes,\n","                    population_size=20, max_generations=30, crossover_rate=0.8,\n","                    mutation_rate=0.15, tournament_size=3, elite_size=2):\n","    \"\"\"Main function to run GA training\"\"\"\n","\n","    # Initialize trainer\n","    trainer = GA_CNN_Trainer(\n","        train_loader=train_loader,\n","        valid_loader=valid_loader,\n","        test_loader=test_loader,\n","        num_classes=num_classes\n","    )\n","\n","    # Run optimization\n","    result = trainer.run_optimization(\n","        population_size=population_size,\n","        max_generations=max_generations,\n","        crossover_rate=crossover_rate,\n","        mutation_rate=mutation_rate,\n","        tournament_size=tournament_size,\n","        elite_size=elite_size,\n","        verbose=True\n","    )\n","\n","    # Test final model\n","    test_accuracy = trainer.test_model(result['final_model'])\n","\n","    # Plot convergence and diversity\n","    plt.figure(figsize=(15, 5))\n","\n","    plt.subplot(1, 3, 1)\n","    plt.plot(result['convergence_history'], 'g-', linewidth=2, marker='o', markersize=4)\n","    plt.title('GA Convergence Curve')\n","    plt.xlabel('Generation')\n","    plt.ylabel('Best Fitness')\n","    plt.grid(True, alpha=0.3)\n","\n","    plt.subplot(1, 3, 2)\n","    # Plot last 10 generations for detail\n","    if len(result['convergence_history']) > 10:\n","        plt.plot(range(len(result['convergence_history'])-10, len(result['convergence_history'])),\n","                result['convergence_history'][-10:], 'g-', linewidth=2, marker='o', markersize=6)\n","        plt.title('GA Convergence (Last 10 Generations)')\n","        plt.xlabel('Generation')\n","        plt.ylabel('Best Fitness')\n","        plt.grid(True, alpha=0.3)\n","\n","    plt.subplot(1, 3, 3)\n","    # Show parameter distribution in final population\n","    final_params = [trainer.vector_to_params(ind) for ind in result['final_population']]\n","    learning_rates = [p['learning_rate'] for p in final_params]\n","    hidden_units = [p['hidden_units'] for p in final_params]\n","\n","    plt.scatter(learning_rates, hidden_units, alpha=0.6, c='green')\n","    plt.xlabel('Learning Rate')\n","    plt.ylabel('Hidden Units')\n","    plt.title('Final Population Distribution\\n(Learning Rate vs Hidden Units)')\n","    plt.xscale('log')\n","    plt.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return result, test_accuracy\n","\n"],"metadata":{"id":"iG1ocdxfVoi6","executionInfo":{"status":"ok","timestamp":1757151120056,"user_tz":-60,"elapsed":101,"user":{"displayName":"Sikirulahi Abdulkareem","userId":"07450201339681726284"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"STARTING GENETIC ALGORITHM HYPERPARAMETER OPTIMIZATION\")\n","print(\"=\"*60)\n","\n","# Configuration 1: Standard GA\n","print(\"\\n1. Running Standard GA Configuration...\")\n","result_standard, test_acc_standard = run_ga_training(\n","    train_loader, valid_loader, test_loader,\n","    num_classes=len(class_names),\n","    population_size=16,\n","    max_generations=14,\n","    crossover_rate=0.8,\n","    mutation_rate=0.15,\n","    tournament_size=3,\n","    elite_size=2\n",")\n","\n","print(f\"\\nStandard GA Results:\")\n","print(f\"Final test accuracy: {test_acc_standard:.4f}\")\n","print(f\"Best parameters: {result_standard['best_params']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TpYifm9Vodd","outputId":"439e7075-c718-42da-d38a-f0cf4c41df26"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","============================================================\n","STARTING GENETIC ALGORITHM HYPERPARAMETER OPTIMIZATION\n","============================================================\n","\n","1. Running Standard GA Configuration...\n","Starting Genetic Algorithm optimization...\n","Population size: 16\n","Dimensions: 5\n","Max generations: 14\n","Crossover rate: 0.8\n","Mutation rate: 0.15\n","--------------------------------------------------\n","Generation   0: Best fitness = 0.094095\n","Generation   5: Best = 0.068130, Avg = 0.369946, Std = 0.288234\n"]}]},{"cell_type":"code","source":["# Configuration 2: High Mutation GA (More Exploration)\n","print(\"\\n2. Running High Mutation GA Configuration...\")\n","result_high_mutation, test_acc_high_mutation = run_ga_training(\n","    train_loader, valid_loader, test_loader,\n","    num_classes=len(class_names),\n","    population_size=20,\n","    max_generations=30,\n","    crossover_rate=0.7,\n","    mutation_rate=0.25,    # Higher mutation rate\n","    tournament_size=4,\n","    elite_size=1\n",")\n","\n","print(f\"\\nHigh Mutation GA Results:\")\n","print(f\"Final test accuracy: {test_acc_high_mutation:.4f}\")\n","print(f\"Best parameters: {result_high_mutation['best_params']}\")"],"metadata":{"id":"gmAz86jRVoa-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configuration 3: Elitist GA (More Exploitation)\n","print(\"\\n3. Running Elitist GA Configuration...\")\n","result_elitist, test_acc_elitist = run_ga_training(\n","    train_loader, valid_loader, test_loader,\n","    num_classes=len(class_names),\n","    population_size=15,\n","    max_generations=35,\n","    crossover_rate=0.9,\n","    mutation_rate=0.08,    # Lower mutation rate\n","    tournament_size=5,     # Larger tournament\n","    elite_size=4          # More elites\n",")\n","\n","print(f\"\\nElitist GA Results:\")\n","print(f\"Final test accuracy: {test_acc_elitist:.4f}\")\n","print(f\"Best parameters: {result_elitist['best_params']}\")"],"metadata":{"id":"QQfc0sLuWMyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configuration 4: Diverse GA (Balanced)\n","print(\"\\n4. Running Diverse GA Configuration...\")\n","result_diverse, test_acc_diverse = run_ga_training(\n","    train_loader, valid_loader, test_loader,\n","    num_classes=len(class_names),\n","    population_size=25,    # Larger population\n","    max_generations=20,\n","    crossover_rate=0.85,\n","    mutation_rate=0.12,\n","    tournament_size=2,     # Smaller tournament for diversity\n","    elite_size=3\n",")\n","\n","print(f\"\\nDiverse GA Results:\")\n","print(f\"Final test accuracy: {test_acc_diverse:.4f}\")\n","print(f\"Best parameters: {result_diverse['best_params']}\")"],"metadata":{"id":"6o79OAsCVoXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FsLN_wODW4FD"},"execution_count":null,"outputs":[]}]}